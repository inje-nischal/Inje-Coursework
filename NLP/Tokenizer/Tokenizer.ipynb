{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461af81-753c-431d-8728-a1bb002076b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02ced4f4-98a3-49ea-ae64-c3468968bbfe",
   "metadata": {},
   "source": [
    "## Program to demonstrate a simple tokenizer\n",
    "\n",
    "Inputs collection of string.\n",
    "\n",
    "Builds vocabulary by splitting on space. **Converts text to lowercase**\n",
    "\n",
    "Encode sentence into list of input ids.\n",
    "\n",
    "Decode list of ids to string\n",
    "\n",
    "Uses <UNK> token for word not present in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "29cf0edc-64c0-48a1-8b6e-b0d31a639beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"Demonstrate simple Tokenizer class in python\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = {\"<UNK>\": 0}\n",
    "        self.id2word = {0: \"<UNK>\"}\n",
    "        self.next_id = 1\n",
    "\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Builds vocabulary from string\"\"\"\n",
    "        for text in texts:\n",
    "            words = text.lower().split(\" \")\n",
    "            unique_words = set(words)\n",
    "            \n",
    "            for word in unique_words:\n",
    "                self.vocab[word] = self.next_id\n",
    "                self.id2word[self.next_id] = word\n",
    "                self.next_id += 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert input text to list of ids\"\"\"\n",
    "        encoded_ids = []\n",
    "        words = text.lower().split(\" \")\n",
    "        for word in words:\n",
    "            encoded_ids.append(self.vocab.get(word, 0))\n",
    "\n",
    "        return encoded_ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert list o ids back to string\"\"\"\n",
    "        words = []\n",
    "        for id in ids:\n",
    "            words.append(self.id2word.get(id, \"<UNK>\"))\n",
    "        return \" \".join(words)\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a013e78-7bfd-4ca7-96c4-d1dab553e897",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d2657066-c4aa-4db3-8305-7cd30a2e1d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Idx: {'<UNK>': 0, 'love': 1, 'i': 2, 'learning': 3, 'ai': 4}\n",
      "Idx2Word: {0: '<UNK>', 1: 'love', 2: 'i', 3: 'learning', 4: 'ai'}\n"
     ]
    }
   ],
   "source": [
    "input_str = [\"I love learning AI\"]\n",
    "\n",
    "simple_tokenizer = Tokenizer()\n",
    "simple_tokenizer.build_vocab(input_str)\n",
    "\n",
    "print(f\"Word2Idx: {simple_tokenizer.vocab}\")\n",
    "print(f\"Idx2Word: {simple_tokenizer.id2word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e7e12-a49f-46d8-a0ee-d8f1b8f4c886",
   "metadata": {},
   "source": [
    "### String to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7eb3d4c0-874b-4915-96b3-3bcd23df5d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [2, 1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "encoded_ids = simple_tokenizer.encode(\"I love learning AI\")\n",
    "print(f\"Encoded IDs: {encoded_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdedd35-57e5-4d38-9bf5-3097af23c3cd",
   "metadata": {},
   "source": [
    "### IDs to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "82a0f869-88ab-44b0-8850-2d008ed97695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: i love learning ai\n"
     ]
    }
   ],
   "source": [
    "string = simple_tokenizer.decode(encoded_ids)\n",
    "print(f\"Decoded text: {string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6efa4-6abb-4e91-88b0-523a33293ae5",
   "metadata": {},
   "source": [
    "### Handling Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5e4a8a3a-6a50-4e46-a4f1-5823cc3f7f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [2, 1, 3, 0]\n",
      "Decoded text: i love learning <UNK>\n"
     ]
    }
   ],
   "source": [
    "input_string = \"I love learning math\"\n",
    "encoded_ids = simple_tokenizer.encode(input_string)\n",
    "print(f\"Encoded IDs: {encoded_ids}\")\n",
    "string = simple_tokenizer.decode(encoded_ids)\n",
    "print(f\"Decoded text: {string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476b57a-e297-4b90-be70-2c8749d65476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5b48f-bc29-414d-9e2a-ce22f59fe7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Inje_Coursework)",
   "language": "python",
   "name": "inje_coursework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
